# ğŸš€ Banesco Data Engineering Stack

Sistema de procesamiento de datos para anÃ¡lisis de bicicletas Divvy con Apache Airflow, Spark, PostgreSQL y MinIO.

## ğŸ“‹ Tabla de Contenidos

- [Requisitos Previos](#-requisitos-previos)
- [ConfiguraciÃ³n del Entorno](#-configuraciÃ³n-del-entorno)
- [Levantar el Stack](#-levantar-el-stack)
- [Ejecutar la SoluciÃ³n](#-ejecutar-la-soluciÃ³n)
- [Servicios Disponibles](#-servicios-disponibles)
- [SoluciÃ³n de Problemas](#-soluciÃ³n-de-problemas)

## ğŸ”§ Requisitos Previos

Antes de comenzar, asegÃºrate de tener instalado:

- **Docker** (versiÃ³n 20.10 o superior)
- **Docker Compose** (versiÃ³n 2.0 o superior)
- **Make** (para usar los comandos del Makefile)
- **Git** (para clonar el repositorio)

### Verificar InstalaciÃ³n

```bash
# Verificar Docker
docker --version
docker-compose --version

# Verificar Make
make --version
```

## âš™ï¸ ConfiguraciÃ³n del Entorno

### 1. Clonar el Repositorio

```bash
git clone <url-del-repositorio>
cd Banesco_Test_Data_Engineer
```

### 2. Verificar Docker

AsegÃºrate de que Docker estÃ© ejecutÃ¡ndose:

```bash
docker info
```

Si no estÃ¡ ejecutÃ¡ndose, inicia Docker Desktop.

### 3. Configurar Variables de Entorno (Opcional)

El sistema funciona con valores por defecto, pero puedes personalizar:

```bash
# Crear archivo .env (opcional)
cp .env.example .env
# Editar las variables segÃºn necesites
```

## ğŸš€ Levantar el Stack

### OpciÃ³n 1: Stack Completo (Recomendado)

```bash
make start
```

Este comando:
- âœ… Levanta todos los servicios (PostgreSQL, MinIO, Spark, Airflow, Streamlit)
- âœ… Configura automÃ¡ticamente las variables y conexiones
- âœ… Crea los buckets necesarios en MinIO
- âœ… Inicia el dashboard de Streamlit

### OpciÃ³n 2: Stack BÃ¡sico

```bash
make start-stack
```

Solo levanta los servicios sin configuraciÃ³n automÃ¡tica.

### Verificar que Todo Funciona

```bash
# Ver estado de todos los contenedores
make status

# Ver logs si hay problemas
make logs
```

## ğŸ¯ Ejecutar la SoluciÃ³n

### 1. Acceder a Airflow

1. Abre tu navegador en: **http://localhost:8080**
2. Usuario: `admin`
3. ContraseÃ±a: `admin`

### 2. Ejecutar el Pipeline

1. **Encontrar el DAG**: Busca `data_bike_pipeline` en la lista de DAGs
2. **Habilitar el DAG**: Haz clic en el toggle para activarlo
3. **Ejecutar**: Haz clic en el botÃ³n "Trigger DAG" (â–¶ï¸)
4. **Monitorear**: Ve el progreso en tiempo real

### 3. Verificar los Resultados

El pipeline procesa datos de bicicletas Divvy y los guarda en:
- **MinIO**: Datos en formato Parquet
- **PostgreSQL**: Datos procesados para anÃ¡lisis

## ğŸŒ Servicios Disponibles

| Servicio | URL | Usuario/ContraseÃ±a | DescripciÃ³n |
|----------|-----|-------------------|-------------|
| **Airflow** | http://localhost:8080 | admin/admin | OrquestaciÃ³n de tareas |
| **Streamlit** | http://localhost:8501 | - | Dashboard de visualizaciÃ³n |
| **Spark Master** | http://localhost:8081 | - | Interfaz de Spark |
| **MinIO** | http://localhost:9001 | minioadmin/minioadmin123 | Almacenamiento de objetos |
| **PostgreSQL** | localhost:5433 | postgres/postgres123 | Base de datos |

### Acceso RÃ¡pido

```bash
# Abrir todos los servicios
make urls
```

## ğŸ” Monitoreo y Logs

### Ver Logs de un Servicio EspecÃ­fico

```bash
# Logs de Airflow
docker logs banesco_airflow_scheduler

# Logs de Spark
docker logs banesco_spark_master

# Logs de MinIO
docker logs banesco_minio
```

### Ver Estado del Sistema

```bash
# Estado de todos los contenedores
make status

# Logs en tiempo real
make logs
```

## ğŸ› ï¸ Comandos Ãštiles

### GestiÃ³n del Stack

```bash
# Iniciar todo
make start

# Parar todo
make stop

# Reiniciar todo
make restart

# Ver estado
make status
```

### Desarrollo

```bash
# Construir imagen de Airflow
make build

# Limpiar contenedores
make clean

# Limpiar TODO (incluyendo datos)
make clean-all
```

### ConfiguraciÃ³n

```bash
# Configurar variables de Airflow
make setup-vars

# Configurar conexiÃ³n PostgreSQL
make setup-postgres-connection

# Configurar conexiÃ³n Spark
make setup-spark-connection
```

## ğŸš¨ SoluciÃ³n de Problemas

### Problema: "Docker no estÃ¡ ejecutÃ¡ndose"

**SoluciÃ³n:**
```bash
# Iniciar Docker Desktop
# O en Linux:
sudo systemctl start docker
```

### Problema: "Puerto ya en uso"

**SoluciÃ³n:**
```bash
# Ver quÃ© estÃ¡ usando el puerto
lsof -i :8080

# Parar el proceso o cambiar puerto en docker-compose.yml
```

### Problema: "Airflow no responde"

**SoluciÃ³n:**
```bash
# Reiniciar Airflow
docker-compose restart airflow-scheduler airflow-webserver

# Verificar logs
docker logs banesco_airflow_scheduler
```

### Problema: "Error de conexiÃ³n a base de datos"

**SoluciÃ³n:**
```bash
# Reconfigurar conexiones
make setup-postgres-connection

# Verificar que PostgreSQL estÃ© corriendo
docker ps | grep postgres
```

### Problema: "Spark no funciona"

**SoluciÃ³n:**
```bash
# Verificar que Spark estÃ© corriendo
docker ps | grep spark

# Reiniciar Spark
docker-compose restart spark-master spark-worker
```

## ğŸ“Š Flujo de Datos

```
1. ğŸ“¥ ExtracciÃ³n: Datos de Divvy Bikes â†’ MinIO (Raw Zone)
2. ğŸ”„ TransformaciÃ³n: Procesamiento con Spark â†’ MinIO (Stage Zone)  
3. ğŸ—„ï¸ Carga: Datos finales â†’ PostgreSQL (Analytics)
4. ğŸ“Š VisualizaciÃ³n: Dashboard en Streamlit
```

## ğŸ†˜ Obtener Ayuda

Si tienes problemas:

1. **Revisa los logs**: `make logs`
2. **Verifica el estado**: `make status`
3. **Consulta la documentaciÃ³n tÃ©cnica**: `README_INFRASTRUCTURE.md`
4. **Reinicia el stack**: `make restart`

## ğŸ‰ Â¡Listo!

Tu stack de Data Engineering estÃ¡ funcionando. Ahora puedes:

- âœ… Ejecutar pipelines de datos en Airflow
- âœ… Visualizar datos en Streamlit
- âœ… Monitorear el procesamiento en Spark
- âœ… Gestionar archivos en MinIO
- âœ… Consultar datos en PostgreSQL

---

**Â¿Necesitas mÃ¡s detalles tÃ©cnicos?** Consulta `README_INFRASTRUCTURE.md` para informaciÃ³n avanzada sobre la arquitectura y configuraciÃ³n.
