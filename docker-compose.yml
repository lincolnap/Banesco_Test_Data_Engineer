version: '3.8'

services:
  # PostgreSQL Database for Banesco Data
  postgres:
    image: postgres:16-alpine
    container_name: banesco_postgres
    restart: unless-stopped
    env_file:
      - ./stack/postgres/.env
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - postgres_config:/docker-entrypoint-initdb.d
    ports:
      - "5433:5432"
    networks:
      - banesco_test
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # PostgreSQL Database for Airflow
  airflow-db:
    image: postgres:16-alpine
    container_name: banesco_airflow_db
    restart: unless-stopped
    env_file:
      - ./stack/airflow-db/.env
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - airflow_db_data:/var/lib/postgresql/data
      - airflow_db_config:/docker-entrypoint-initdb.d
    ports:
      - "5434:5432"
    networks:
      - banesco_test
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # MinIO Object Storage
  minio:
    image: minio/minio:latest
    container_name: banesco_minio
    restart: unless-stopped
    env_file:
      - ./stack/minio/.env
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    volumes:
      - minio_data:/data
      - minio_config:/root/.minio
    ports:
      - "9000:9000"
      - "9001:9001"
    networks:
      - banesco_test
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  # Apache Airflow - Database Initialization
  airflow-init:
    build:
      context: ./stack/airflow
      dockerfile: Dockerfile
    container_name: banesco_airflow_init
    env_file:
      - ./stack/airflow/airflow.env
    environment:
      AIRFLOW_UID: ${AIRFLOW_UID}
      AIRFLOW_GID: ${AIRFLOW_GID}
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: ${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
      AIRFLOW__API__AUTH_BACKENDS: ${AIRFLOW__API__AUTH_BACKENDS}
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: ${AIRFLOW__WEBSERVER__EXPOSE_CONFIG}
      AIRFLOW__WEBSERVER__EXPOSE_HOSTNAME: ${AIRFLOW__WEBSERVER__EXPOSE_HOSTNAME}
      AIRFLOW__WEBSERVER__EXPOSE_STACKTRACE: ${AIRFLOW__WEBSERVER__EXPOSE_STACKTRACE}
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: ${AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK}
      AIRFLOW__LOGGING__LOGGING_LEVEL: ${AIRFLOW__LOGGING__LOGGING_LEVEL}
      AIRFLOW__LOGGING__FAB_LOGGING_LEVEL: ${AIRFLOW__LOGGING__FAB_LOGGING_LEVEL}
    volumes:
      - airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
    networks:
      - banesco_test
    depends_on:
      airflow-db:
        condition: service_healthy
    command: >
      bash -c "airflow db init && 
               airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@banesco.com --password admin"
    profiles:
      - init

  # Apache Airflow - Scheduler
  airflow-scheduler:
    build:
      context: ./stack/airflow
      dockerfile: Dockerfile
    container_name: banesco_airflow_scheduler
    restart: unless-stopped
    env_file:
      - ./stack/airflow/airflow.env
    environment:
      AIRFLOW_UID: ${AIRFLOW_UID}
      AIRFLOW_GID: ${AIRFLOW_GID}
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: ${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
      AIRFLOW__API__AUTH_BACKENDS: ${AIRFLOW__API__AUTH_BACKENDS}
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: ${AIRFLOW__WEBSERVER__EXPOSE_CONFIG}
      AIRFLOW__WEBSERVER__EXPOSE_HOSTNAME: ${AIRFLOW__WEBSERVER__EXPOSE_HOSTNAME}
      AIRFLOW__WEBSERVER__EXPOSE_STACKTRACE: ${AIRFLOW__WEBSERVER__EXPOSE_STACKTRACE}
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: ${AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK}
      AIRFLOW__LOGGING__LOGGING_LEVEL: ${AIRFLOW__LOGGING__LOGGING_LEVEL}
      AIRFLOW__LOGGING__FAB_LOGGING_LEVEL: ${AIRFLOW__LOGGING__FAB_LOGGING_LEVEL}
    volumes:
      - airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
    networks:
      - banesco_test
    depends_on:
      airflow-db:
        condition: service_healthy
    command: airflow scheduler
    healthcheck:
      test: ["CMD-SHELL", "[ -f /opt/airflow/airflow-webserver.pid ]"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Apache Airflow - Webserver
  airflow-webserver:
    build:
      context: ./stack/airflow
      dockerfile: Dockerfile
    container_name: banesco_airflow_webserver
    restart: unless-stopped
    env_file:
      - ./stack/airflow/airflow.env
    environment:
      AIRFLOW_UID: ${AIRFLOW_UID}
      AIRFLOW_GID: ${AIRFLOW_GID}
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: ${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
      AIRFLOW__API__AUTH_BACKENDS: ${AIRFLOW__API__AUTH_BACKENDS}
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: ${AIRFLOW__WEBSERVER__EXPOSE_CONFIG}
      AIRFLOW__WEBSERVER__EXPOSE_HOSTNAME: ${AIRFLOW__WEBSERVER__EXPOSE_HOSTNAME}
      AIRFLOW__WEBSERVER__EXPOSE_STACKTRACE: ${AIRFLOW__WEBSERVER__EXPOSE_STACKTRACE}
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: ${AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK}
      AIRFLOW__LOGGING__LOGGING_LEVEL: ${AIRFLOW__LOGGING__LOGGING_LEVEL}
      AIRFLOW__LOGGING__FAB_LOGGING_LEVEL: ${AIRFLOW__LOGGING__FAB_LOGGING_LEVEL}
    volumes:
      - airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
    ports:
      - "8080:8080"
    networks:
      - banesco_test
    depends_on:
      airflow-db:
        condition: service_healthy
    command: airflow webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Apache Spark - Master
  spark-master:
    image: bitnami/spark:3.5
    container_name: banesco_spark_master
    restart: unless-stopped
    env_file:
      - ./stack/spark/.env
    environment:
      SPARK_MODE: ${SPARK_MODE}
      SPARK_RPC_AUTHENTICATION_ENABLED: ${SPARK_RPC_AUTHENTICATION_ENABLED}
      SPARK_RPC_ENCRYPTION_ENABLED: ${SPARK_RPC_ENCRYPTION_ENABLED}
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: ${SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED}
      SPARK_SSL_ENABLED: ${SPARK_SSL_ENABLED}
      SPARK_MASTER_HOST: ${SPARK_MASTER_HOST}
      SPARK_MASTER_PORT: ${SPARK_MASTER_PORT}
      SPARK_MASTER_WEBUI_PORT: ${SPARK_MASTER_WEBUI_PORT}
      SPARK_WORKER_WEBUI_PORT: ${SPARK_WORKER_WEBUI_PORT}
    volumes:
      - spark_data:/opt/bitnami/spark/data
      - spark_config:/opt/bitnami/spark/conf
    ports:
      - "7077:7077"
      - "8081:8080"
    networks:
      - banesco_test
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Apache Spark - Worker
  spark-worker:
    image: bitnami/spark:3.5
    container_name: banesco_spark_worker
    restart: unless-stopped
    env_file:
      - ./stack/spark/.env
    environment:
      SPARK_MODE: worker
      SPARK_RPC_AUTHENTICATION_ENABLED: ${SPARK_RPC_AUTHENTICATION_ENABLED}
      SPARK_RPC_ENCRYPTION_ENABLED: ${SPARK_RPC_ENCRYPTION_ENABLED}
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: ${SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED}
      SPARK_SSL_ENABLED: ${SPARK_SSL_ENABLED}
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_WEBUI_PORT: ${SPARK_WORKER_WEBUI_PORT}
    volumes:
      - spark_data:/opt/bitnami/spark/data
      - spark_config:/opt/bitnami/spark/conf
    ports:
      - "8082:8081"
    networks:
      - banesco_test
    depends_on:
      - spark-master
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  banesco_test:
    driver: bridge
    name: banesco_test

volumes:
  postgres_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/postgres/data
  postgres_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/postgres/init
  airflow_db_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/airflow-db/data
  airflow_db_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/airflow-db/init
  minio_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/minio/data
  minio_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/minio/config
  airflow_dags:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/airflow/dags
  airflow_logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/airflow/logs
  airflow_plugins:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/airflow/plugins
  spark_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/spark/data
  spark_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/spark/config
