version: '3.8'

services:
  # PostgreSQL Database for Banesco Data
  postgres:
    image: postgres:16-alpine
    container_name: banesco_postgres
    restart: unless-stopped
    env_file:
      - ./stack/postgres/.env
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - postgres_config:/docker-entrypoint-initdb.d
    ports:
      - "5433:5432"
    networks:
      - banesco_test
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # PostgreSQL Database for Airflow
  airflow-db:
    image: postgres:16-alpine
    container_name: banesco_airflow_db
    restart: unless-stopped
    env_file:
      - ./stack/airflow-db/.env
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - airflow_db_data:/var/lib/postgresql/data
      - airflow_db_config:/docker-entrypoint-initdb.d
    ports:
      - "5434:5432"
    networks:
      - banesco_test
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5


  # Kafka Message Broker (KRaft Mode)
  kafka:
    image: bitnami/kafka:latest
    container_name: banesco_kafka
    restart: unless-stopped
    environment:
      KAFKA_CFG_NODE_ID: 0
      KAFKA_CFG_PROCESS_ROLES: controller,broker
      KAFKA_CFG_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
      KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,EXTERNAL://localhost:9094
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: 0@kafka:9093
      KAFKA_CFG_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: true
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_MIN_INSYNC_REPLICAS: 1
      KAFKA_NUM_PARTITIONS: 3
    volumes:
      - kafka_data:/bitnami/kafka
    ports:
      - "9092:9092"
      - "9094:9094"
    networks:
      - banesco_test
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server localhost:9092 --list"]
      interval: 30s
      timeout: 10s
      retries: 3

  # MongoDB
  mongodb:
    image: mongo:7
    container_name: banesco_mongodb
    restart: unless-stopped
    env_file:
      - ./stack/mongodb/.env
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_INITDB_ROOT_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_INITDB_ROOT_PASSWORD}
      MONGO_INITDB_DATABASE: ${MONGO_INITDB_DATABASE}
    volumes:
      - mongodb_data:/data/db
    ports:
      - "27017:27017"
    networks:
      - banesco_test
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5

  # MinIO Object Storage
  minio:
    image: minio/minio:latest
    container_name: banesco_minio
    restart: unless-stopped
    env_file:
      - ./stack/minio/.env
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    volumes:
      - minio_data:/data
      - minio_config:/root/.minio
    ports:
      - "9000:9000"
      - "9001:9001"
    networks:
      - banesco_test
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  # Apache Airflow - Database Initialization
  airflow-init:
    image: apache/airflow:2.9.3
    container_name: banesco_airflow_init
    env_file:
      - ./stack/airflow/.env
    environment:
      AIRFLOW_UID: ${AIRFLOW_UID}
      AIRFLOW_GID: ${AIRFLOW_GID}
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: ${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
      AIRFLOW__API__AUTH_BACKENDS: ${AIRFLOW__API__AUTH_BACKENDS}
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: ${AIRFLOW__WEBSERVER__EXPOSE_CONFIG}
      AIRFLOW__WEBSERVER__EXPOSE_HOSTNAME: ${AIRFLOW__WEBSERVER__EXPOSE_HOSTNAME}
      AIRFLOW__WEBSERVER__EXPOSE_STACKTRACE: ${AIRFLOW__WEBSERVER__EXPOSE_STACKTRACE}
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: ${AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK}
      AIRFLOW__LOGGING__LOGGING_LEVEL: ${AIRFLOW__LOGGING__LOGGING_LEVEL}
      AIRFLOW__LOGGING__FAB_LOGGING_LEVEL: ${AIRFLOW__LOGGING__FAB_LOGGING_LEVEL}
    volumes:
      - airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
    networks:
      - banesco_test
    depends_on:
      airflow-db:
        condition: service_healthy
    command: >
      bash -c "airflow db init && 
               airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@banesco.com --password admin"
    profiles:
      - init

  # Apache Airflow - Scheduler
  airflow-scheduler:
    image: apache/airflow:2.9.3
    container_name: banesco_airflow_scheduler
    restart: unless-stopped
    env_file:
      - ./stack/airflow/.env
    environment:
      AIRFLOW_UID: ${AIRFLOW_UID}
      AIRFLOW_GID: ${AIRFLOW_GID}
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: ${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
      AIRFLOW__API__AUTH_BACKENDS: ${AIRFLOW__API__AUTH_BACKENDS}
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: ${AIRFLOW__WEBSERVER__EXPOSE_CONFIG}
      AIRFLOW__WEBSERVER__EXPOSE_HOSTNAME: ${AIRFLOW__WEBSERVER__EXPOSE_HOSTNAME}
      AIRFLOW__WEBSERVER__EXPOSE_STACKTRACE: ${AIRFLOW__WEBSERVER__EXPOSE_STACKTRACE}
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: ${AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK}
      AIRFLOW__LOGGING__LOGGING_LEVEL: ${AIRFLOW__LOGGING__LOGGING_LEVEL}
      AIRFLOW__LOGGING__FAB_LOGGING_LEVEL: ${AIRFLOW__LOGGING__FAB_LOGGING_LEVEL}
    volumes:
      - airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
    networks:
      - banesco_test
    depends_on:
      airflow-db:
        condition: service_healthy
    command: airflow scheduler
    healthcheck:
      test: ["CMD-SHELL", "[ -f /opt/airflow/airflow-webserver.pid ]"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Apache Airflow - Webserver
  airflow-webserver:
    image: apache/airflow:2.9.3
    container_name: banesco_airflow_webserver
    restart: unless-stopped
    env_file:
      - ./stack/airflow/.env
    environment:
      AIRFLOW_UID: ${AIRFLOW_UID}
      AIRFLOW_GID: ${AIRFLOW_GID}
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: ${AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
      AIRFLOW__API__AUTH_BACKENDS: ${AIRFLOW__API__AUTH_BACKENDS}
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: ${AIRFLOW__WEBSERVER__EXPOSE_CONFIG}
      AIRFLOW__WEBSERVER__EXPOSE_HOSTNAME: ${AIRFLOW__WEBSERVER__EXPOSE_HOSTNAME}
      AIRFLOW__WEBSERVER__EXPOSE_STACKTRACE: ${AIRFLOW__WEBSERVER__EXPOSE_STACKTRACE}
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: ${AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK}
      AIRFLOW__LOGGING__LOGGING_LEVEL: ${AIRFLOW__LOGGING__LOGGING_LEVEL}
      AIRFLOW__LOGGING__FAB_LOGGING_LEVEL: ${AIRFLOW__LOGGING__FAB_LOGGING_LEVEL}
    volumes:
      - airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
    ports:
      - "8080:8080"
    networks:
      - banesco_test
    depends_on:
      airflow-db:
        condition: service_healthy
    command: airflow webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Apache Spark - Master
  spark-master:
    image: bitnami/spark:3.5
    container_name: banesco_spark_master
    restart: unless-stopped
    env_file:
      - ./stack/spark/.env
    environment:
      SPARK_MODE: ${SPARK_MODE}
      SPARK_RPC_AUTHENTICATION_ENABLED: ${SPARK_RPC_AUTHENTICATION_ENABLED}
      SPARK_RPC_ENCRYPTION_ENABLED: ${SPARK_RPC_ENCRYPTION_ENABLED}
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: ${SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED}
      SPARK_SSL_ENABLED: ${SPARK_SSL_ENABLED}
      SPARK_MASTER_HOST: ${SPARK_MASTER_HOST}
      SPARK_MASTER_PORT: ${SPARK_MASTER_PORT}
      SPARK_MASTER_WEBUI_PORT: ${SPARK_MASTER_WEBUI_PORT}
      SPARK_WORKER_WEBUI_PORT: ${SPARK_WORKER_WEBUI_PORT}
    volumes:
      - spark_data:/opt/bitnami/spark/data
      - spark_config:/opt/bitnami/spark/conf
    ports:
      - "7077:7077"
      - "8081:8080"
    networks:
      - banesco_test
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Apache Spark - Worker
  spark-worker:
    image: bitnami/spark:3.5
    container_name: banesco_spark_worker
    restart: unless-stopped
    env_file:
      - ./stack/spark/.env
    environment:
      SPARK_MODE: worker
      SPARK_RPC_AUTHENTICATION_ENABLED: ${SPARK_RPC_AUTHENTICATION_ENABLED}
      SPARK_RPC_ENCRYPTION_ENABLED: ${SPARK_RPC_ENCRYPTION_ENABLED}
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: ${SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED}
      SPARK_SSL_ENABLED: ${SPARK_SSL_ENABLED}
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_WEBUI_PORT: ${SPARK_WORKER_WEBUI_PORT}
    volumes:
      - spark_data:/opt/bitnami/spark/data
      - spark_config:/opt/bitnami/spark/conf
    ports:
      - "8082:8081"
    networks:
      - banesco_test
    depends_on:
      - spark-master
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Streamlit Application
  streamlit:
    build:
      context: ./stack/streamlit
      dockerfile: Dockerfile
    container_name: banesco_streamlit
    restart: unless-stopped
    env_file:
      - ./stack/streamlit/.env
    environment:
      STREAMLIT_SERVER_PORT: ${STREAMLIT_SERVER_PORT}
      STREAMLIT_SERVER_ADDRESS: ${STREAMLIT_SERVER_ADDRESS}
      STREAMLIT_SERVER_HEADLESS: ${STREAMLIT_SERVER_HEADLESS}
      STREAMLIT_BROWSER_GATHER_USAGE_STATS: ${STREAMLIT_BROWSER_GATHER_USAGE_STATS}
      STREAMLIT_SERVER_ENABLE_CORS: ${STREAMLIT_SERVER_ENABLE_CORS}
    volumes:
      - streamlit_app:/app
    ports:
      - "8502:8501"
    networks:
      - banesco_test
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/_stcore/health"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  banesco_test:
    driver: bridge
    name: banesco_test

volumes:
  postgres_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/postgres/data
  postgres_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/postgres/init
  kafka_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/kafka/data
  mongodb_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/mongodb/data
  minio_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/minio/data
  minio_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/minio/config
  airflow_dags:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/airflow/dags
  airflow_logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/airflow/logs
  airflow_plugins:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/airflow/plugins
  airflow_db_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/airflow-db/data
  airflow_db_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/airflow-db/init
  spark_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/spark/data
  spark_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/spark/config
  streamlit_app:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./stack/streamlit/app
